\documentclass{article}

% !TeX spellcheck = en_US 

\title{CS457 Project 5}
\author{Nick Rohde}
\date{31$^{st}$ May 2018}

\usepackage{subcaption}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage[labelfont=bf]{caption}
\usepackage{mathbbol}
\usepackage{float}

\hypersetup
{
	colorlinks=true,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=blue,
	linktoc=all,
	linkcolor=blue,
}

\begin{document}
\maketitle
\tableofcontents
\pagebreak

\section{Introduction} \label{S1}
	For this project, we compared three different classifiers in an image recognition problem. The three classifiers we used for this were the Multilayer Perceptron (MLP), and RBF Network (RBF), and a Support Vector Machine (SVM). \\
	
	For each classifier, $7~\times~7$ images in the form of binary matrices were used to train and test the classifier's ability to recognize the letters A through J. The training and testing sets both contained some amount of random noise to see how this would effect the classification accuracy of the different classifiers. \\
	
	We will see that, overall, the support vector machine outperformed the other two classifiers, though all did extremely well in this problem.


\section{Experimentation} \label{S2}
	In this section, we will briefly discuss the experimentation that was done with the parameters of each of the three classifiers. The decisions discussed here were used to generate the data displayed in \hyperref[S5]{Section 5} and discussed in \hyperref[S3]{Section 3}. The parameters that are not discussed below were left at their default value as they had no significant impact on the classifier's accuracy. \\
	
	\begin{minipage}{\linewidth}
		\centering
		\captionof{table}{Overall Test Configurations}
		\begin{tabular}{c|cc}\label{Test_Setup}
			Set Number 	& \# of samples (per letter)	& maximum noise (\%) \\
			\hline
			1					&  100 			 	&	10				 \\
			2					&  50				&	4					\\ 
			3					&  20	 			&	20					\\
		\end{tabular}
	\end{minipage}
	
	\subsection{Overview}
		For all the classifiers, the same data was used for training and testing. This data was generated by taking the prototype for each letter (displayed in \hyperref[S6]{Section 6}) and then randomly flipping max\_noise bits in the matrix, n\_samples of these data points were generated for each data set. This data set was then shuffled and given to WEKA as input. The size of the set and the amount of noise in the data was randomly assigned before generating the sets. The values used for testing can be found in \hyperref[Test_Setup]{Table 1}\\
		
		Note: Due to the random generation of the data, some input vectors appeared multiple times in the data sets. As it is realistic for data to be replicated, the uniqueness of each input vector was not enforced, resulting in some letters having more training samples than others.\\
		
	
	\subsection{MLP}
		Experiments with the MLP involved three variables, namely, the number of hidden layers, the number of nodes in each layer, and learning rate. We will briefly discuss each one of these; the final values used for the data generation can be found in \hyperref[MLP_In]{Table 2} .\\
		
		During the experiments it became clear that the number of hidden layers has a very strong effect on the results of the classifier. The MLP was tested with one, two, and three hidden layers, which showed that the number of hidden layers depends on the amount of noise in the data. With 4\% and 10\% noise, a single hidden layer was enough, however, with 20\% noise, a second layer had to be added to achieve an acceptable accuracy.\\
		
		The number of hidden neurons was apparently not related to the noise. It appears that, instead, the number of hidden neurons is linked to the number of classes. In our problem, we had 10 classes, and the MLP always performed best when the number of hidden neurons was between 50-70\% of that, i.e. 5-7 hidden neurons. Having more than 8 neurons per hidden layer resulted in a quick decay of accuracy, similarly, having fewer than 5 also resulted in a very low accuracy. \\
		
		Finally, the learning rate was also quite interesting. We have done a similar project in the past and in that project we determined that a learning rate of 0.4 was ideal for this type of problem with 3x3 and 5x5 images; unsurprisingly, this did not change here. Once again, 0.4 achieved the best results for all noise and training set levels. Interestingly, a rate of 0.95 also performed very well as long as the noise was relatively low. So, it appears as though this parameter is inversely proportional to the amount of noise in the data, with more noise implying a lower learn rate. \\
	
		\begin{minipage}{\linewidth}
			\centering
			\captionof{table}{MLP Configurations}
			\begin{tabular}{c|ccc}\label{MLP_In}
				Set Number 	& Hidden Layers	& Hidden Neurons & Learning Rate  \\
				\hline
				1					&  1 					&	7						& 0.4		 \\
				2					&  1					&	5		 				& 0.4		\\ 
				3					&  2					&	7, 7					& 0.4		\\
			\end{tabular}
		\end{minipage}
	
	\subsection{SVM}
		The SVM also used three variables that were experimented with, namely, the Kernel, the complexity constant, and the normalization/standardization of the data. The final values of these variables can be found in \hyperref[SVM_In]{Table 3}; we will briefly discuss the reasoning behind each of these variables' value.\\
		
		The kernel was fairly interesting to experiment with, all of the kernels that WEKA provides were able to provide good results, however, the Normalized Poly Kernel and Poly Kernel provided the overall best performance once the other parameters were tuned properly. The Poly Kernel generally performed slightly better than the Normalized Poly Kernel, except for the first data set, though the Normalized Poly Kernel only resulted in a 0.5\% accuracy increase here. For the other two sets, the Poly Kernel resulted in roughly 5\% higher accuracy.\\
		
		The complexity constant did not appear to have a significant impact on the accuracy of the classifier at first, however, data set 3 performed much better with a complexity constant of 2.0. For sets 1 and 2, on the other hand, varying this constant from the default value of 1.0 did not have any impact on the final results of the classifier. \\
		
		Lastly, the normalization or standardization of the data set did not usually provide an increase in accuracy, with the exception of data set 2. In sets 1 and 3, turning either of these on resulted in a massive decrease in accuracy (around 30\%), whereas for set 2 it increased accuracy by about 5\%. \\

	
	\begin{minipage}{\linewidth}
		\centering
		\captionof{table}{SVM Configurations}
		\begin{tabular}{c|cccc}\label{SVM_In}
			Set Number 	& Kernel Name & Kernel Exponent	& Normalization/Standardization & Complexity Constant  \\
			\hline
			1					&  Normalized Poly Kernel & 2.0 &	None						& 1.0	  \\
			2					&  Poly Kernel & 1.0 					& Standardized		 	  & 0.9		\\ 
			3					&  Poly Kernel & 2.5					& None						  & 2.0		\\
		\end{tabular}
	\end{minipage}

	\subsection{RBF Network}
		The RBF Network used two variables that were altered for experimentation, namely the cluster size for k-means and the standard deviation for clustering. We will discuss the values of these briefly, and the final values are displayed in \hyperref[RBF_In]{Table 4}. \\

		The number of clusters to use for this algorithm was a very interesting tuning parameter. For the smaller data sets, this did not significantly impact speed, however, for set 1 this increased the runtime by about 5 seconds. At first, it appeared that the number of clusters is simply the number of classes in the classification problem; however, this is not entirely correct. Data set 3, which had high amount of noise, performed much better with only 2 clusters, instead of 10 which resulted in the best performance for the other two. Interestingly, with 10 clusters the classifier performed worse than random chance on set 3, only managing a 7.5\% accuracy (6/80 correct) -- random chance would be 10\%. \\
		
		The standard deviation was another interesting parameter. At first, it appeared as though it was related to the noise in the data, however, towards the end it started to appear as though it was instead related to the number of classes. Even though the amount of noise and the size of the data set changed, the standard deviation that was ideal for the set stayed roughly the same. Thus, it appears as though this is related to the number of classes, as this remained constant throughout all trials.\\
				
		
		\begin{minipage}{\linewidth}
			\centering
			\captionof{table}{RBF Configurations}
			\begin{tabular}{c|cc}\label{RBF_In}
				Set Number 	& Cluster Size & Minimum Standard Deviation \\
				\hline
				1					&  10					& 1.42	  \\
				2					&  10					& 1.40	\\ 
				3					&  2					& 1.40	\\
			\end{tabular}
		\end{minipage}

\pagebreak
\section{Analysis}\label{S3}
	In this section, we will briefly discuss the overall trend of the three strategies, and then examine the \hyperref[S5]{Data}.\\
	
	\subsection{Overview}
		Overall, we can see that the support-vector machine (SVM) significantly outperformed the other two strategies. As can be seen in \hyperref[OverallRes]{table 5}, the SVM always did at least as good as the other two classifiers or better. On top of this, the SVM also achieved a much faster training and classification time, compared to RBF it was almost 32 times faster and compared to MLP it was almost four times faster on average. This extra time investment is not even remotely worth it as we can see in Tables \hyperref[MLPAcc]{6}, \hyperref[SVMAcc]{7}, and \hyperref[RBFAcc]{8}, where we see that the SVM also achieved higher average accuracy and a much higher average Kappa value, where MLP and RBF were both at about 0.91 whereas SVM got 0.94. \\


	\subsection{Effect of Noise}
		From the tables \hyperref[MLPAcc]{6}, \hyperref[SVMAcc]{7}, and \hyperref[RBFAcc]{8} we can see that all three classifiers did worse for set 3 than with the other two sets. This was the set with the highest amount of noise. It appears as though set 1, which had 10\% noise, was still very easy for all methods to accurately classify, so there appears to be a threshold somewhere between 10 and 20\% noise where the classifiers run into trouble. With the jump from 10\% to 20\% noise, all classifiers dropped over 10\% in accuracy, whereas the jump from 4\% to 10\% was barely noticeable, even though the latter represented a 150\% increase and the former a 100\% increase.\\
		
	
	\subsection{Confusion Matrices}
		Another interesting thing to look at are the confusion matrices that can be found in \hyperref[S521]{Section 5.2.1}, \hyperref[S531]{Section 5.3.1}, and \hyperref[S541]{Section 5.4.1}. The confusion matrices for sets 1 and 2 are not very interesting, we can see that the classifiers have almost only 0s except on the main-diagonal. However, for set 3, we can see that the values become very scattered with the large amount of noise. What we see here is that only the MLP and SVM were able to at least classify some of each character correctly, the RBF was not able to classify 'Cs' correctly, classifying most of them as 'Es'. Here we can also see that the MLP and SVM had almost identical results for this set, with only 3 classifications being different, namely, 'C', 'G', and 'J'. It appears that These three are the letters that presented the most problems due to their similarity with other characters. \\
		
	
	\subsection{Conclusions}
		Overall, the top performance for this problem was SVM, with MLP coming in as a close second, and RBF falling quite far behind. Though MLP appeared to handle noise slightly better with some characters than SVM, building SVM is much faster than MLP. When we look at the average accuracy gain per second ($\frac{average~accuracy}{average~build~time}$) we can really see where SVM pulls ahead of the rest; MLP provides 57.47\% accuracy per second, SVM provides 224.81\% accuracy per second, and RBF only provides 6.87\% accuracy per second. So, the fact that MLP and SVM are very close together in performance actually acts against the use of MLP here because SVM can build the model in a fraction of the time, making it far more scalable than MLP. \\
		
	\pagebreak
\section{Implementation}\label{S4}
	All tests were run using the WEKA implementations of the MLP (weka.functions.MultilayerPerceptron), the RBF (weka.functions.RBFNetwork) and the SVM (weka.functions.SMO). The test samples were generated using a python script which is included. \\
	
	The exact functions that were used are (including all parameters):\\
	\begin{enumerate}
		\item MLP:
		\begin{enumerate}
			\item[Set 1:] weka.classifiers.functions.MultilayerPerceptron -L 0.4 -M 0.2 -N 500 -V 0 -S 0 -E 20 -H 7
			\item[Set 2:] weka.classifiers.functions.MultilayerPerceptron -L 0.4 -M 0.2 -N 500 -V 0 -S 0 -E 20 -H 5
			\item[Set 3:] weka.classifiers.functions.MultilayerPerceptron -L 0.4 -M 0.2 -N 500 -V 0 -S 0 -E 20 -H "7, 7"
		\end{enumerate}
		\item SVM:
		\begin{enumerate}
			\item[Set 1:] weka.classifiers.functions.SMO -C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 -K\\ "weka.classifiers.functions.supportVector.NormalizedPolyKernel -C 250007 -E 2.0"
			\item[Set 2:] weka.classifiers.functions.SMO -C 0.9 -L 0.001 -P 1.0E-12 -N 1 -V -1 -W 1 -K\\ "weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0"
			\item[Set 3:] weka.classifiers.functions.SMO -C 2.0 -L 0.001 -P 1.0E-12 -N 2 -V -1 -W 1 -K\\ "weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 2.5"
		\end{enumerate}
		\item RBF:
		\begin{enumerate}
			\item[Set 1:] weka.classifiers.functions.RBFNetwork -B 10 -S 1 -R 1.0E-8 -M -1 -W 1.42
			\item[Set 2:] weka.classifiers.functions.RBFNetwork -B 10 -S 1 -R 1.0E-8 -M -1 -W 1.42
			\item[Set 3:] weka.classifiers.functions.RBFNetwork -B 2 -S 1 -R 1.0E-9 -M -1 -W 1.4
		\end{enumerate}
	\end{enumerate}

\pagebreak
\section{Data} \label{S5}
	This section displays the numerical data from the experimentation, discussed in the \hyperref[S3]{Analysis section}.\\

	\subsection{Overview}\label{S51}
		For all data in this section, the same test samples were used. The samples were randomly generated from the prototypes displayed in \hyperref[S6]{Section 6}; the prototype was duplicated and then max\_noise \% bits in the vector were flipped.\\
		The confusion matrices show correct identifications down the main diagonal; the columns represent the category the classifier returned, and the rows are the expected output (e.g.: index (7,4) represents the number of images identified as class 7 (G), that were actually class 4 (D)).\\
		
		\begin{minipage}{\linewidth}
			\centering
			\captionof{table}{Computation comparison for MLP, SVM, and RBF}
				\begin{tabular}{c|ccc|ccc}\label{OverallRes}
					Set Number & \multicolumn{3}{c}{Accuracy (\%)}	& \multicolumn{3}{c}{Build Time (s)} \\
					& MLP & SVM & RBF & MLP & SVM & RBF \\
					\hline
					1			& 97.75 & 98.75 & 98.5 & 2.71 & 0.23 & 39.97 	\\
					2			& 99.5 & 99.5 & 99 & 1.42 & 0.08 & 0.17  \\
					3			& 83.75 & 85 & 78.75 & 0.75 & 0.11 & 0.06 	\\
					\hline
					Mean: &93.67 & 94.42 & 92.08 & 1.63 &0.42&13.4 \\
				\end{tabular}
		\end{minipage}
		
		\subsection{MLP}
			This section displays the test results for the MLP. First, the confusion matrices are shown followed by the final accuracy of testing.
			
			\subsubsection{Classifier Accuracy} \label{S521}
				\[
				Confusion~Matrix_{1} = 
				\begin{bmatrix}
				36& 0& 0& 0& 0 &0 & 0 & 0 & 0 & 0\\
				0& 40&  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
				0&  0& 32&  0&  5&  0 & 0 & 0 & 0 & 0\\
				0 & 0 & 0 &35  &0 & 0&  0&  0&  0&  0\\
				0 & 0 & 2 & 0 &35&  0 & 0 & 0 & 0 & 0\\
				0 & 0 & 0 & 0 & 1 &43&  0&  0&  0&  0\\
				0 & 0 & 1 & 0 & 0 & 0 &40&  0&  0&  0\\
				0 & 0&  0&  0&  0 & 0 & 0 &43 & 0 & 0\\
				0 & 0 & 1 & 0 & 0 & 0&  0&  0& 46 & 0\\
				0 & 0&  0&  0&  0&  0&  0&  0&  0& 40
				\end{bmatrix}
				\]
				\[
				Confusion~Matrix_{2} = 
				\begin{bmatrix}
				20&0&0&0&0&0&0&0&0&0\\
				0&20&0&0&0&0&0&0&0&0\\
				0&0&22&0&1&0&0&0&0&0\\
				0&0&0&17&0&0&0&0&0&0\\
				0&0&0&0&18&0&0&0&0&0\\
				0&0&0&0&0&23&0&0&0&0\\
				0&0&0&0&0&0&22&0&0&0\\
				0&0&0&0&0&0&0&16&0&0\\
				0&0&0&0&0&0&0&0&22&0\\
				0&0&0&0&0&0&0&0&0&19 
				\end{bmatrix}
				\]
				\[
				Confusion~Matrix_{3} = 
				\begin{bmatrix}
				11&0&0&0&0&0&0&1&0&0\\
				0&6&0&0&0&0&0&0&0&0\\
				0&0&6&3&1&0&1&0&1&0\\
				0&0&0&7&0&0&0&0&0&0\\
				0&0&0&0&3&0&0&0&0&0\\
				0&0&0&0&0&10&0&0&0&0\\
				0&0&2&0&1&0&2&0&1&0\\
				0&0&0&0&0&0&0&7&0&0\\
				0&0&0&0&0&0&0&0&7&0\\
				0&1&0&0&0&0&0&0&1&8
				\end{bmatrix}
				\]
				\linebreak
				
				\begin{minipage}{\linewidth}
					\centering
					\captionof{table}{MLP Accuracy}
					\begin{tabular}{c|cc}\label{MLPAcc}
						Set Number 	& \# Correctly Identified	& Kappa Statistic \\
						\hline
						1			&  391 (of 400)		 		&	0.9341					 \\
						2			&  199 (of 200)		 		&	0.9944				 \\ 
						3			&  67 (of 80)		 		&	0.8182					 \\
						\hline
						Mean: &93.66\% &0.9156
					\end{tabular}
				\end{minipage}
				
		\subsection{SVM}
		This section displays the test results for the SVM. First, the confusion matrices are shown followed by the final accuracy of testing.
		
			\subsubsection{Classifier Accuracy} \label{S531}
			\[
			Confusion~Matrix_{1} = 
			\begin{bmatrix}
			36&0&0&0&0&0&0&0&0&0\\
			0&40&0&0&0&0&0&0&0&0\\
			0&0&34&0&3&0&0&0&0&0\\
			0&0&0&35&0&0&0&0&0&0\\
			0&0&1&0&36&0&0&0&0&0\\
			0&0&0&0&0&44&0&0&0&0\\
			0&0&1&0&0&0&40&0&0&0\\
			0&0&0&0&0&0&0&43&0&0\\
			0&0&0&0&0&0&0&0&47&0\\
			0&0&0&0&0&0&0&0&0&40
			\end{bmatrix}
			\]
			\[
			Confusion~Matrix_{2} = 
			\begin{bmatrix}
			20&0&0&0&0&0&0&0&0&0\\
			0&20&0&0&0&0&0&0&0&0\\
			0&0&22&0&1&0&0&0&0&0\\
			0&0&0&17&0&0&0&0&0&0\\
			0&0&0&0&18&0&0&0&0&0\\
			0&0&0&0&0&23&0&0&0&0\\
			0&0&0&0&0&0&22&0&0&0\\
			0&0&0&0&0&0&0&16&0&0\\
			0&0&0&0&0&0&0&0&22&0\\
			0&0&0&0&0&0&0&0&0&19
			\end{bmatrix}
			\]
			\[
			Confusion~Matrix_{3} = 
			\begin{bmatrix}
			12&0&0&0&0&0&0&0&0&0\\
			0&6&0&0&0&0&0&0&0&0\\
			0&0&3&2&5&0&0&0&2&0\\
			0&0&0&7&0&0&0&0&0&0\\
			0&0&0&0&3&0&0&0&0&0\\
			0&0&0&0&0&10&0&0&0&0\\
			0&0&1&0&2&0&3&0&0&0\\
			0&0&0&0&0&0&0&7&0&0\\
			0&0&0&0&0&0&0&0&7&0\\
			0&0&0&0&0&0&0&0&0&10\\
			\end{bmatrix}
			\]
			\linebreak
			
			\begin{minipage}{\linewidth}
				\centering
				\captionof{table}{SVM Accuracy}
				\begin{tabular}{c|cc}\label{SVMAcc}
					Set Number 	& \# Correctly Identified	& Kappa Statistic \\
					\hline
					1			&  395 (of 400)		 		&	0.9861						 \\
					2			&  199 (of 200)		 		&	0.9944				 \\ 
					3			&  68 (of 80)		 		&	0.833					 \\
					\hline
					Mean: & 94.41\% &0.9378
				\end{tabular}
			\end{minipage}
				
		\subsection{RBF}
			This section displays the test results for the RBF. First, the confusion matrices are shown followed by the final accuracy of testing.
		
			\subsubsection{Classifier Accuracy}	 \label{S541}
			\[
			Confusion~Matrix_{1} = 
			\begin{bmatrix}
			12&0&0&0&0&0&0&0&0&0\\
			0&6&0&0&0&0&0&0&0&0\\
			0&0&3&2&5&0&0&0&2&0\\
			0&0&0&7&0&0&0&0&0&0\\
			0&0&0&0&3&0&0&0&0&0\\
			0&0&0&0&0&10&0&0&0&0\\
			0&0&1&0&2&0&3&0&0&0\\
			0&0&0&0&0&0&0&7&0&0\\
			0&0&0&0&0&0&0&0&7&0\\
			0&0&0&0&0&0&0&0&0&10
			\end{bmatrix}
			\]
			\[
			Confusion~Matrix_{2} = 
			\begin{bmatrix}
			20&0&0&0&0&0&0&0&0&0\\
			0&20&0&0&0&0&0&0&0&0\\
			0&0&21&0&1&0&1&0&0&0\\
			0&0&0&17&0&0&0&0&0&0\\
			0&0&0&0&18&0&0&0&0&0\\
			0&0&0&0&0&23&0&0&0&0\\
			0&0&0&0&0&0&22&0&0&0\\
			0&0&0&0&0&0&0&16&0&0\\
			0&0&0&0&0&0&0&0&22&0\\
			0&0&0&0&0&0&0&0&0&19
			\end{bmatrix}
			\]
			\[
			Confusion~Matrix_{3} = 
			\begin{bmatrix}
			12&0&0&0&0&0&0&0&0&0\\
			0&6&0&0&0&0&0&0&0&0\\
			0&0&0&1&11&0&0&0&0&0\\
			0&0&0&7&0&0&0&0&0&0\\
			0&0&0&0&3&0&0&0&0&0\\
			0&0&0&0&0&10&0&0&0&0\\
			0&0&0&0&2&0&4&0&0&0\\
			0&0&0&0&0&0&0&7&0&0\\
			0&0&0&0&0&0&0&0&7&0\\
			0&2&0&0&0&0&0&0&1&7\\
			\end{bmatrix}
			\]
			\linebreak

			\begin{minipage}{\linewidth}
				\centering
				\captionof{table}{RBF Accuracy}
				\begin{tabular}{c|cc}\label{RBFAcc}
					Set Number 	& \# Correctly Identified	& Kappa Statistic \\
					\hline
					1			&  394 (of 400)		 		&	0.9833						 \\
					2			&  198 (of 200)		 		&	0.9889				 \\ 
					3			&  63 (of 80)		 		&	0.7657					 \\
					\hline
					Mean: & 92.08\% &0.9126
				\end{tabular}
			\end{minipage}
	
	\section{Prototypes}\label{S6}
	The following matrices were used as prototypes for generating the samples for each of the 3 strategies in discussed above. The matrices were converted into vectors for training by appending the rows together. In the matrices below, the blank spaces represent 0s, they have been omitted to make the character easier to see. 
	
	\[
	A = 
	\begin{bmatrix}
	&&1&1&1&&\\
	&&1&&1&&\\
	&1&&&&1&\\
	&1&1&1&1&1&\\
	1&&&&&&1\\
	1&&&&&&1\\
	1&&&&&&1
	\end{bmatrix}
	~B = 
	\begin{bmatrix}
	&1&1&1&&&\\
	&1&&&1&&\\
	&1&&&1&&\\
	&1&1&1&&&\\
	&1&&&1&&\\
	&1&&&1&&\\
	&1&1&1&&&
	\end{bmatrix}
	~C = 
	\begin{bmatrix}
	&1&1&1&1&1&\\
	&1&&&&&\\
	&1&&&&&\\
	&1&&&&&\\
	&1&&&&&\\
	&1&&&&&\\
	&1&1&1&1&1&
	\end{bmatrix}
	~D = 
	\begin{bmatrix}
	&1&1&1&1&&\\
	&1&&&&1&\\
	&1&&&&&1\\
	&1&&&&&1\\
	&1&&&&&1\\
	&1&&&&1&\\
	&1&1&1&1&&
	\end{bmatrix}
	\]
	\[
	E = 
	\begin{bmatrix}
	&1&1&1&1&1&\\
	&1&&&&&\\
	&1&&&&&\\
	&1&1&1&&&\\
	&1&&&&&\\
	&1&&&&&\\
	&1&1&1&1&1&
	\end{bmatrix}
	~F = 
	\begin{bmatrix}
	&&1&1&1&1&\\
	&&1&&&&\\
	&&1&&&&\\
	&&1&1&1&&\\
	&&1&&&&\\
	&&1&&&&\\
	&&1&&&&
	\end{bmatrix}
	~G = 
	\begin{bmatrix}
	&1&1&1&1&1&\\
	&1&&&&&\\
	&1&&&&&\\
	&1&&1&1&1&\\
	&1&&&&1&\\
	&1&&&&1&\\
	&1&1&1&1&1&
	\end{bmatrix}
	~H = 
	\begin{bmatrix}
	&1&&&&1&\\
	&1&&&&1&\\
	&1&&&&1&\\
	&1&1&1&1&1&\\
	&1&&&&1&\\
	&1&&&&1&\\
	&1&&&&1&
	\end{bmatrix}
	\]
	\[
	I = 
	\begin{bmatrix}
	&&1&1&1&&\\
	&&&1&&&\\
	&&&1&&&\\
	&&&1&&&\\
	&&&1&&&\\
	&&&1&&&\\
	&&1&1&1&&
	\end{bmatrix}
	~J = 
	\begin{bmatrix}
	&1&1&1&1&&\\
	&&&&1&&\\
	&&&&1&&\\
	&&&&1&&\\
	&&&&1&&\\
	&1&&&1&&\\
	&&1&1&&&
	\end{bmatrix}
	\]

		


\end{document}